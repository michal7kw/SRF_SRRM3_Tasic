{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Same for Tasic 2016 (SRP061902)\n",
    "wget -O tasic2016_metadata.tsv \"https://www.ebi.ac.uk/ena/portal/api/filereport?accession=SRP061902&result=read_run&fields=run_accession,fastq_ftp&format=tsv&download=true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "cat tasic2016_metadata.tsv | tail -n +2 | while IFS=$'\\t' read -r run_acc fastq_urls; do\n",
    "    IFS=';' read -ra urls <<< \"$fastq_urls\"\n",
    "    for url in \"${urls[@]}\"; do\n",
    "        echo \"wget -c 'ftp://$url'\"\n",
    "    done\n",
    "done > download_commands_2016.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checks for existing files before adding them to the download list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Extract SRR numbers and create download commands, checking for existing files\n",
    "cat tasic2016_metadata.tsv | tail -n +2 | while IFS=$'\\t' read -r run_acc fastq_urls; do\n",
    "    IFS=';' read -ra urls <<< \"$fastq_urls\"\n",
    "    for url in \"${urls[@]}\"; do\n",
    "        # Extract filename from URL\n",
    "        filename=$(basename \"$url\")\n",
    "        # Check if file exists and is not empty\n",
    "        if [ ! -s \"$filename\" ]; then\n",
    "            echo \"wget -c 'ftp://$url'\"\n",
    "        else\n",
    "            echo \"# Skipping $filename - already exists\" >&2\n",
    "        fi\n",
    "    done\n",
    "done > download_commands_2016.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add an MD5 check if you want to verify existing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Extract SRR numbers and create download commands, checking for existing files\n",
    "cat tasic2016_metadata.tsv | tail -n +2 | while IFS=$'\\t' read -r run_acc fastq_urls md5s; do\n",
    "    IFS=';' read -ra urls <<< \"$fastq_urls\"\n",
    "    IFS=';' read -ra checksums <<< \"$md5s\"\n",
    "    \n",
    "    for i in \"${!urls[@]}\"; do\n",
    "        url=\"${urls[$i]}\"\n",
    "        md5=\"${checksums[$i]}\"\n",
    "        filename=$(basename \"$url\")\n",
    "        \n",
    "        # Check if file exists and has correct MD5\n",
    "        if [ -f \"$filename\" ]; then\n",
    "            existing_md5=$(md5sum \"$filename\" | cut -d' ' -f1)\n",
    "            if [ \"$existing_md5\" = \"$md5\" ]; then\n",
    "                echo \"# Skipping $filename - already exists with correct MD5\" >&2\n",
    "                continue\n",
    "            else\n",
    "                echo \"# $filename exists but MD5 mismatch - will redownload\" >&2\n",
    "            fi\n",
    "        fi\n",
    "        echo \"wget -c 'ftp://$url'\"\n",
    "    done\n",
    "done > download_commands_2016.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use HTTPS instead of FTP, which is generally more reliable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "cat tasic2016_metadata.tsv | tail -n +2 | while IFS=$'\\t' read -r run_acc fastq_urls; do\n",
    "    IFS=';' read -ra urls <<< \"$fastq_urls\"\n",
    "    for url in \"${urls[@]}\"; do\n",
    "        # Convert FTP URL to HTTPS\n",
    "        https_url=$(echo \"$url\" | sed 's|^|https://ftp.sra.ebi.ac.uk/|')\n",
    "        echo \"wget -c '$https_url'\" \n",
    "    done\n",
    "done > download_commands_2016.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "chmod +x download_commands.sh\n",
    "\n",
    "parallel -j 32 < download_commands_2016.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "ls -R logs/ | head -n 10\n",
    "cat -n 10 logs/SRR7318040/*_mapping.log\n",
    "cat -n 10 logs/SRR7318040/gapless.log\n",
    "cat -n 10 logs/SRR7318040/countit.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "conda create -n quantas python=3.7\n",
    "conda activate quantas\n",
    "conda install -c conda-forge glibc=2.29\n",
    "conda install -c chaolinzhanglab quantas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "# Create necessary directories if they don't exist\n",
    "mkdir -p cache gapless_out countit_out logs genome_index/olego\n",
    "\n",
    "# First ensure we have the genome index and required files\n",
    "if [ ! -f \"genome_index/olego/mm10/mm10.fa\" ]; then\n",
    "    echo \"Need to setup genome index first. Download mm10 and run: olegoindex -a bwtsw mm10.fa\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Function to process a single paired-end sample\n",
    "process_sample() {\n",
    "    local srr=$1\n",
    "    local logdir=\"logs/${srr}\"\n",
    "    mkdir -p \"$logdir\"\n",
    "\n",
    "    echo \"Processing ${srr}...\"\n",
    "\n",
    "    # 1. Mapping reads with OLego - note using proper file paths as per documentation\n",
    "    echo \"Mapping ${srr} read 1...\"\n",
    "    olego -v -t 16 \\\n",
    "        -j mm10.intron.hmr.bed \\\n",
    "        -o ${srr}_1.sam \\\n",
    "        genome_index/olego/mm10/mm10.fa \\\n",
    "        tasic2018/raw_data/${srr}_1.fastq.gz \\\n",
    "        2> \"${logdir}/${srr}_1_mapping.log\"\n",
    "\n",
    "    echo \"Mapping ${srr} read 2...\"\n",
    "    olego -v -t 16 \\\n",
    "        -j mm10.intron.hmr.bed \\\n",
    "        -o ${srr}_2.sam \\\n",
    "        genome_index/olego/mm10/mm10.fa \\\n",
    "        tasic2018/raw_data/${srr}_2.fastq.gz \\\n",
    "        2> \"${logdir}/${srr}_2_mapping.log\"\n",
    "\n",
    "    # 2. Process paired-end reads - note the correct file path for isoform database\n",
    "    echo \"Running gapless on ${srr}...\"\n",
    "    perl gapless/gapless_huge_file.pl -v -sam -uniq \\\n",
    "        --split-size 10000000 \\\n",
    "        -isoform mm10/mm10.exon.trio.hmr.nr.bed \\\n",
    "        -E 400 -big --print-singleton \\\n",
    "        --library-type unstranded \\\n",
    "        -o gapless_out/${srr} \\\n",
    "        ${srr}_1.sam ${srr}_2.sam \\\n",
    "        2> \"${logdir}/${srr}_gapless.log\"\n",
    "\n",
    "    # 3. Quantify splicing - using mm10.conf from your mm10 directory\n",
    "    echo \"Quantifying splicing for ${srr}...\"\n",
    "    perl countit/summarize_splicing_wrapper.pl \\\n",
    "        -c ./cache -v -big -weight \\\n",
    "        -conf mm10/mm10.conf -dbkey mm10 \\\n",
    "        -cass -taca -alt5 -alt3 -mutx -iret \\\n",
    "        gapless_out/${srr}/pair.gapless.bed \\\n",
    "        countit_out/${srr} \\\n",
    "        2> \"${logdir}/${srr}_countit.log\"\n",
    "\n",
    "    # Clean up intermediate files if they exist\n",
    "    if [ -f \"${srr}_1.sam\" ]; then rm \"${srr}_1.sam\"; fi\n",
    "    if [ -f \"${srr}_2.sam\" ]; then rm \"${srr}_2.sam\"; fi\n",
    "}\n",
    "\n",
    "# Get list of unique SRR IDs from your fastq files\n",
    "srr_ids=$(ls tasic2018/raw_data/*_1.fastq.gz | sed 's/.*\\///' | sed 's/_1.fastq.gz//' | sort | uniq)\n",
    "\n",
    "# Process each sample\n",
    "for srr in $srr_ids; do\n",
    "    if ! process_sample $srr; then\n",
    "        echo \"Processing failed for ${srr}, check logs directory\"\n",
    "        exit 1\n",
    "    fi\n",
    "done\n",
    "\n",
    "# After processing all samples, create visualization files\n",
    "for srr in $srr_ids; do\n",
    "    echo \"Creating visualization files for ${srr}...\"\n",
    "    perl quantas/countit/tag2profile.pl -v -big -weight -c ./cache \\\n",
    "        -exact -of bedgraph -n \"${srr}_profile\" \\\n",
    "        gapless_out/${srr}/pair.gapless.bed \\\n",
    "        ${srr}.bedGraph\n",
    "\n",
    "    perl quantas/countit/tag2junction.pl -v -big -weight -c ./cache \\\n",
    "        gapless_out/${srr}/pair.gapless.bed \\\n",
    "        ${srr}.junction.count.bed\n",
    "done"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
